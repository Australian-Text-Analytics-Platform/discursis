{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95b82e17-e6fd-43b9-9a91-6082d8358160",
   "metadata": {},
   "source": [
    "# Analysis of Social Media Data using Discursis\n",
    "\n",
    "In this workshop, we'll look at how we can use Discursis to analyze conversation and discourse\n",
    "transcripts, and other text that can be structured like a conversation, such as social media data.\n",
    "\n",
    "The first dataset we'll use is the transcript of the National Press Club Leaders Debate\n",
    "between Kevin Rudd and Tony Abbott, available at the [Parliament of Australia website](https://parlinfo.aph.gov.au/parlInfo/search/display/display.w3p;query=Id:%22media/pressrel/2658246%22) under a [CC BY-NC-ND 3.0 AU](https://creativecommons.org/licenses/by-nc-nd/3.0/au/)\n",
    "Creative Commons license.\n",
    "\n",
    "The second type of dataset we will investigate is youtube data as collected and prepared via [Youte](https://github.com/QUT-Digital-Observatory/youte/). This will show some of the challenges of analysing social media data as conversations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b97febb-9869-4f88-8fc5-21353a2f0de3",
   "metadata": {},
   "source": [
    "## Python setup\n",
    "\n",
    "Discursis and the accompanying tools for conversation data are in the [atap_widgets](https://github.com/Australian-Text-Analytics-Platform/atap_widgets) Python\n",
    "package. We'll load the tools from this library, along with the other libraries we'll be using\n",
    "for the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0d38a-6574-4534-918c-cac3237eed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# In order for bokeh to work properly on Binder, we need to specify\n",
    "#   the URL it's running on\n",
    "os.environ[\"BINDER_EXTERNAL_URL\"] = os.environ.get('JUPYTERHUB_EXTERNAL_URL', \"https://notebooks.gesis.org/\")\n",
    "\n",
    "# pandas: tools for data processing\n",
    "import pandas as pd\n",
    "\n",
    "# numpy: tools for numerical calculations\n",
    "import numpy as np\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "# Bokeh: interactive plots\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColorBar\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.transform import linear_cmap\n",
    "\n",
    "# networkx: graphs/networks\n",
    "import networkx as nx\n",
    "\n",
    "# matplotlib: another plotting library\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This needs to be run to enable interactive bokeh plots\n",
    "output_notebook()\n",
    "\n",
    "# Individual tools from atap_widgets\n",
    "from atap_widgets.conversation import (\n",
    "    ConceptSimilarityModel,\n",
    "    Conversation,\n",
    "    EmbeddingModel,\n",
    ")\n",
    "from atap_widgets.plotting import ConversationPlot\n",
    "from atap_widgets.concordance import (\n",
    "    ConcordanceTable,\n",
    "    ConcordanceWidget,\n",
    "    prepare_text_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a922feaf-f379-476a-b7f5-1d773e2c0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a results folder, if it doesn't already exist\n",
    "os.makedirs(\"conversation_results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9791a7-d1d0-4ec1-ab8f-f27f6527f7cc",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "The conversation tools are designed to accept data as a `pandas` dataframe.\n",
    "Each row in the dataframe should be an utterance in the conversation. There\n",
    "should be a `\"text\"` column with the actual content of the utterance\n",
    "and a `\"speaker\"` column identifying who is speaking. It also helps if we have a `\"text_id\"` column that gives a unique identifier for each utterance that\n",
    "we can refer to.\n",
    "\n",
    "\n",
    "Additional metadata columns that might be relevant to your particular dataset\n",
    "will be imported into the conversation tool as-is. In this case we have an additional\n",
    "`\"role\"` column identifying each person's role in the debate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b03f58-ac05-407b-a675-5d7ecc41cefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"data/debate_clean.xlsx\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed5439d-9642-42c6-9d68-7f4c1506a138",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b657b6ce-303c-4fa2-91b6-5cd5e32c9492",
   "metadata": {},
   "source": [
    "Before carrying out our analysis using Discursis, we can do some initial exploration of the data\n",
    "to get an overview of it. We can do this with the tools that are built in to `pandas`,\n",
    "rather than specialized language tools.\n",
    "\n",
    "We can see how many times each person spoke during the debate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b107bf1-1eb2-4b58-a8bb-2e85dd185065",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total utterances:\", len(data))\n",
    "data[\"speaker\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef0751-5074-49d1-89aa-be672d7f12a6",
   "metadata": {},
   "source": [
    "And see how many times people in different roles spoke:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d8ebcd-1a40-494b-a81b-62836b0d85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(data[\"speaker\"], data[\"role\"], margins=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34174180-bc06-49c9-a8ec-fba8cba74ce5",
   "metadata": {},
   "source": [
    "## Concordance\n",
    "\n",
    "We can use the `ConcordanceWidget` tool to start looking at some key terms\n",
    "in the conversation. Terms that might be relevant to a political debate\n",
    "might be things like \"economy\" or \"environment\".\n",
    "\n",
    "Before using the concordance tools, we need to use the `prepare_text_df()`\n",
    "function to process the data, which applies some initial NLP processing.\n",
    "\n",
    "If you have other ideas about relevant topics, you can use the \n",
    "`ConcordanceWidget` to search for them in real-time. If you've\n",
    "found useful results, you can export them to Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb8056-38f1-4f7a-89d8-b96f8d5202ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = prepare_text_df(data, text_column=\"text\", id_column=\"text_id\")\n",
    "widget = ConcordanceWidget(data, results_per_page=10)\n",
    "widget.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acf6c28-4035-4fcb-b96b-a25794a77930",
   "metadata": {},
   "source": [
    "## Analysis of Conversation Data\n",
    "\n",
    "In order to model our conversation data, we need to load our data into\n",
    "a `Conversation` object. The `Conversation` object carries out some initial processing of the text,\n",
    "which will be handled by a `spacy` language model. If you need to analyse\n",
    "data for a non-English corpus, you can install a relevant [spacy model](https://spacy.io/usage/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a684da-ffb5-4a31-a24a-18240a0fadb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = Conversation(\n",
    "    data=data,\n",
    "    text_column=\"text\",\n",
    "    speaker_column=\"speaker\",\n",
    "    id_column=\"text_id\",\n",
    "    language_model=\"en_core_web_sm\",\n",
    ")\n",
    "conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2522c42-a8c7-46a2-8e7e-61e8b2fff279",
   "metadata": {},
   "source": [
    "The `Conversation` object offers some basic functionality for accessing information about\n",
    "the conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cab589-630f-4ec7-9880-f1a63c6dd203",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.n_speakers, conversation.n_utterances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d748a46c-b397-40c9-ac57-41cdd2a9a6fc",
   "metadata": {},
   "source": [
    "You can access the table of utterance data via `conversation.data` - this\n",
    "is a `pandas` DataFrame like the original data but has some additional\n",
    "information added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37595cba-ea3f-4ef0-b232-c11ea79e0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9454a83-ea3e-45fe-a597-e1d287a551df",
   "metadata": {},
   "source": [
    "When we calculate conversational(semantic) similarity below, the default method is based\n",
    "on the most common terms in the data. We can check what these are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b2c713-acc1-4896-b773-1f1647843dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.get_most_common_terms(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7711ce18-a785-466c-8163-badacddb2b41",
   "metadata": {},
   "source": [
    "We may want to treat some of these terms as **stopwords** so that they don't contribute to\n",
    "the calculation of topic similarity. Stopwords are common grammatical terms that don't carry specific\n",
    "contextual meaning, such as `and, of, the, is`. Note that the `spacy` language model we are using already has some\n",
    "default stopwords defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707ac1a-0830-4c43-ba26-f5443e2c49bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_stopwords = sorted(conversation.nlp.Defaults.stop_words)\n",
    "print(len(default_stopwords), \"default stopwords\")\n",
    "default_stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc3ea4-ee57-4470-8acc-5e074b7123a9",
   "metadata": {},
   "source": [
    "After adding these stopwords,\n",
    "the changes should be applied in any new operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e73d9-07cd-44f7-994a-950efb5486b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.add_stopword(\"mr\")\n",
    "conversation.add_stopword(\"said\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ee50df-cc35-4fa4-89e5-33fe85081e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.get_most_common_terms(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b32ad14-7bf2-4721-aca6-dc67950d82fb",
   "metadata": {},
   "source": [
    "You can also access and export the full frequency table of terms. The `term_frequencies` table\n",
    "is a `pandas` DataFrame, so we can export it to Excel easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669cf3d7-f1d0-4eac-991a-b3aa66bbff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequencies = conversation.get_term_frequencies()\n",
    "term_frequencies.to_excel(\"conversation_results/term_frequencies.xlsx\", index=False)\n",
    "term_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b54fea5-b4ae-4dfc-9c88-3324d94a7a7a",
   "metadata": {},
   "source": [
    "### Calculating similarity\n",
    "\n",
    "In order to calculate similarity of terms and topics across the conversation,\n",
    "we'll use the conceptual recurrence calculation from\n",
    "\n",
    "> Angus, D., Smith, A. E., & Wiles, J. (2012). Human Communication as Coupled Time Series: Quantifying Multi-Participant Recurrence. IEEE Transactions on Audio, Speech, and Language Processing, 20(6), 1795â€“1807. https://doi.org/10.1109/TASL.2012.2189566\n",
    "\n",
    "This method is implemented in `ConceptSimilarityModel`, which takes in a conversation\n",
    "object and performs the similarity calculation on it. To match the method used in the article,\n",
    "we'll use the top 50 key terms as the basis for concepts, which we set with the\n",
    "`key_terms` argument, and use 3-sentence windows when counting which terms co-occur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c3f98-43f9-474d-ba6f-51c26741a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_model = ConceptSimilarityModel(\n",
    "    conversation, key_terms=50, sentence_window_size=3\n",
    ")\n",
    "\n",
    "concept_similarity = concept_model.get_conversation_similarity()\n",
    "concept_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7656831a-d6bc-4bfc-89c8-444d58012541",
   "metadata": {},
   "source": [
    "For convenience, we only need to call one function to get the utterance-to-utterance\n",
    "similarity that will form the basis of the Discursis plot:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1894ee5-00ec-4022-a12d-b961dba59ed9",
   "metadata": {},
   "source": [
    "## Visualizing similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e07130-dc2f-48a5-a68b-886ce4ae422c",
   "metadata": {},
   "source": [
    "The Discursis-style plot of similarity across the conversation is\n",
    "available through `ConversationPlot`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0604ff5a-4f5d-47a0-885e-f6a82d119038",
   "metadata": {},
   "source": [
    "You can use the **Box Zoom** tool to zoom in on parts of the plot manually.\n",
    "\n",
    "**Box zoom icon**: ![Box zoom icon](https://docs.bokeh.org/en/latest/_images/BoxZoom.png)\n",
    "\n",
    "You can also adjust the\n",
    "`threshold` option to remove tiles with low similarity, to better highlight the utterances\n",
    "that are similar - note that the default is to not apply a threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67920e40-98d0-4bbd-960b-e49ae12c3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "discursis_plot = ConversationPlot(\n",
    "    conversation, \n",
    "    similarity_matrix=concept_similarity,\n",
    "    threshold=0.2,\n",
    ")\n",
    "discursis_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68d23f-ba39-41f6-9242-773d0edf5f88",
   "metadata": {},
   "source": [
    "## Using Discursis with YouTube data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1555f5a-3354-419c-ae37-05354f2e78c5",
   "metadata": {},
   "source": [
    "To apply Discursis to the YouTube data we've already collected and prepared using `youte tidy` requires:\n",
    "\n",
    "1. (If working in Binder or other JupyterHub instances) Uploading the processed database to the binder instance.\n",
    "2. Loading the comment data from the prepared database.\n",
    "3. Telling Discursis which columns to use in the dataset.\n",
    "4. Applying the functions we used earlier to visualise the conversation, and use the keyword tools to find concordance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00c417a",
   "metadata": {},
   "source": [
    "### Collecting YouTube data\n",
    "\n",
    "For this section, we're going to look at the comments on a [YouTube video discussing a cat related videogame](https://www.youtube.com/watch?v=PVYVkXhQtzU). To collect and prepare the comments using [youte](https://github.com/QUT-Digital-Observatory/youte/), we can run the following commands:\n",
    "\n",
    "```\n",
    "# Collect the top level comments on the specific video id\n",
    "youte list-comments -v kitty.json PVYVkXhQtzU\n",
    "youte tidy kitty.json kitty.db\n",
    "\n",
    "# Collect the comments in a specific thread\n",
    "youte list-comments -t kitty_thread.json UgzlxcyHprauGpVcsBV4AaABAg\n",
    "youte tidy kitty_thread.json kitty_thread.db\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7c69d0",
   "metadata": {},
   "source": [
    "### Uploading Data\n",
    "\n",
    "Data can be uploaded using the standard Jupyter tools - in this case we're going to upload the collected data to the `data` subdirectory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f514d81",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "To load data we will connect directly to the uploaded database and load what we need from there. Note that we're ordering results by `published_at` - this establishes a sequence of comments for discursis to use when visualising the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9351c184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create a connection to the database\n",
    "conn = sqlite3.connect(\"data/kitty_thread.db\")\n",
    "\n",
    "# Note that we need to order by a data - as we need the *sequence* of comments on the page \n",
    "# Resetting \n",
    "data = pd.read_sql_query(\"select * from comments order by published_at\", conn)\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746a8869",
   "metadata": {},
   "source": [
    "### Preparing the Conversation\n",
    "\n",
    "As before, we'll prepare the conversation object - note that we're mapping the YouTube `author_name` as the \"speaker\", and the `text_original` column as the text for Discursis to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f2302",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = Conversation(\n",
    "    data=data,\n",
    "    text_column=\"text_original\",\n",
    "    speaker_column=\"author_name\",\n",
    "    id_column=\"index\",\n",
    "    language_model=\"en_core_web_sm\",\n",
    ")\n",
    "conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c139f0",
   "metadata": {},
   "source": [
    "#### Investigation\n",
    "\n",
    "As before let's take a look at the common terms in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6fdb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.get_most_common_terms(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b34b39",
   "metadata": {},
   "source": [
    "And you'll notice that \"etc\" is a common word, so let's remove that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b21650",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.add_stopword(\"etc\")\n",
    "conversation.get_most_common_terms(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1a9ec2",
   "metadata": {},
   "source": [
    "### Create and Visualising the Similarity Model\n",
    "\n",
    "We'll follow the same steps as earlier to create and then visualise the similarity model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c7ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_model = ConceptSimilarityModel(\n",
    "    conversation, key_terms=50, sentence_window_size=3\n",
    ")\n",
    "\n",
    "concept_similarity = concept_model.get_conversation_similarity()\n",
    "concept_model.key_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479239de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discursis_plot = ConversationPlot(\n",
    "    conversation, \n",
    "    similarity_matrix=concept_similarity,\n",
    "    threshold=0.5,\n",
    ")\n",
    "discursis_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409ca78d",
   "metadata": {},
   "source": [
    "### Looking Deeper with Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_text_df(data, text_column=\"text_original\", id_column=\"index\")\n",
    "widget = ConcordanceWidget(data, results_per_page=10)\n",
    "widget.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
